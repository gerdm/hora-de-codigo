{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hora de C칩digo: Ense침ando a Aprender\n",
    "## El pipeline de data science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos nos encomiendan la tarea de escribir una sequela para el Libro Don Quixote de la Mancha. Debido a nuestra poca experiencia en obras literarias, y la inifinitesimal probabilidad de que hayamos le칤do la obra en su totalidad.\n",
    "\n",
    "![DonQuijote](https://www.telesurtv.net/__export/1421342197589/sites/telesur/img/multimedia/2015/01/15/quijote.jpg_1718483347.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para lograr el objetivo, ser치 necesario dividir la tarea en tres partes\n",
    "\n",
    "* **An치lisis**\n",
    "* **Modelaci칩n**\n",
    "* **Producci칩n**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import requests\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from unidecode import unidecode\n",
    "from collections import Counter, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An치lisis\n",
    "----\n",
    "## Primeros Pasos\n",
    "\n",
    "Con el fin de escribir una sequela, lo primero que realizaremos ser치 entender la primera parte del libro.\n",
    "\n",
    "**쮻e qu칠 manera podemos obtener acceso a la obra?**  \n",
    "1. Transcribir el libro a nuestra computadora \n",
    "2. Buscar el libro en l칤nea, copiarlo y pegarlo en alg칰n lugar para tener acceso a este\n",
    "3. Acceder directamente al libro y no tener que copiar y pegar nada (游녨)\n",
    "\n",
    "Para nuestra suerte, la p치gina _[Project Gutemberg](http://www.gutenberg.org)_ ofrece libros gratuitos en l칤nea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin necesidad de acceder explicitamente a la p치gina, podemos guardar el libro _Don Quijote_ por Miguel de Cervantes Saavedra con las siguientes l칤neas de c칩digo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.gutenberg.org/cache/epub/2000/pg2000.txt\"\n",
    "r = requests.get(url)\n",
    "# Dentro de esta variable guardamos el texto\n",
    "corpus = r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que el texto que acabamos de descargar cuenta con informaci칩n adicional al libro, limpiamos los datos a fin de acotar los datos a analizar y simplificar el an치lisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En las siguientes dos l칤neas de c칩digo buscamos el inicio y el final del libro\n",
    "init_book = corpus.find(\"En un lugar de la Mancha\")\n",
    "end_book = corpus.find(\"End of Project Gutenberg's\")\n",
    "# Acotamos el libro\n",
    "text = corpus[init_book: end_book]\n",
    "# Removemos acentos y eliminamos salltos de l칤nas\n",
    "text = unidecode(text.replace(\"\\r\\n\", \" \")).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en un lugar de la mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que vivia un hidalgo de los de lanza en astillero, adarga antigua, rocin flaco y galgo corredor. una olla de algo mas vaca que carnero, salpicon las mas noches, duelos y quebrantos los sabados, lantejas los viernes, algun palomino de anadidura los domingos, consumian las tres partes de su hacienda. el resto della concluian sayo de velarte, calzas de velludo para las fiestas, con sus pantuflos de lo mesmo, y los dias'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observamos los primeros 500 car치cteres del libro\n",
    "text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El an치lisis que haremos a continuaci칩n consiste en analizar de una manera delimitada la manera en la que Miguel de Cervantes escribi칩 el libro. Para esto, consideraremos cada una de las palabras dentro del texto y las guardaremos dentro de un arreglo ordenado de elementos conocido como una lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['en',\n",
       " 'un',\n",
       " 'lugar',\n",
       " 'de',\n",
       " 'la',\n",
       " 'mancha,',\n",
       " 'de',\n",
       " 'cuyo',\n",
       " 'nombre',\n",
       " 'no',\n",
       " 'quiero',\n",
       " 'acordarme,']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = text.split()\n",
    "tokens[:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la informaci칩n manipulada hasta el momento, ser칤a una buena idea ver qu칠 palabras son las que m치s se repiten dentro del texto.\n",
    "\n",
    "**쯈u칠 palabras esperar칤amos que se repitieran un mayor n칰mero de veces?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contamos cada una de de las palabras dentro de la lista\n",
    "word_counter = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('que', 19813),\n",
       " ('de', 17969),\n",
       " ('y', 16289),\n",
       " ('la', 10199),\n",
       " ('a', 9641),\n",
       " ('el', 9086),\n",
       " ('en', 8019),\n",
       " ('no', 5706),\n",
       " ('se', 5002),\n",
       " ('los', 4688)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observamos los 10 elemento que m치s se repiten\n",
    "word_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos acceder al n칰mero de veces que se repite una palabra en espec칤fico de la siguiente manera:\n",
    "```python\n",
    "word_counter[\"palabra\"]\n",
    "```\n",
    "D칩nde `\"palabra\"` es la palabra a buscar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counter[\"dulcinea\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**쮺u치ntas veces se repite la palabra `\"quijote\"`?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**쮺u치ntas veces se repite la palabra `\"amigo\"`?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando el resultado de `word_counter.most_common(10)`, vemos que las palabras que m치s se repiten son redundantes para hacer un an치lisis del texto.\n",
    "\n",
    "Dentro del archivo `\"spanish_stop.pkl\"` guardamos una lista con palabras redundantes en espa침ol. Corre la siguiente celda. **쯈u칠 observas?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_stopwords = pickle.load(open(\"spanish_stop.pkl\", \"rb\"))\n",
    "spanish_stopwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente celda filtrar치 los elementos que se encuentren dentro de la variable `spanish_stopwords`. **쯈u칠 palabras crees se repetir치n m치s bajo este contexto?**\n",
    "\n",
    "Corre la siguiente celda para averiguar las 10 palabras que m치s se repiten, filtrando todas aquellas palabras que se encuentren deentro de la lista `spanish_stopwords`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tokens = [t for t in tokens if t not in spanish_stopwords]\n",
    "clean_counter = Counter(clean_tokens)\n",
    "clean_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hagamos una im치gen con las palabras que m치s se repiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "wordcloud = WordCloud().generate(\" \".join(clean_tokens))\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Los $n$-grams\n",
    "Saber las palabras que m치s se repiten no ofrece mucho contexto sobre la trama del libro. A fin de obtener un poco m치s de contexto sobre el libro contaremos _pares_ ordenados de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El siguiente c칩digo llena la lista \"elements\" con pares de\n",
    "# palabras dentro del libro\n",
    "elements = []\n",
    "for w0, w1 in zip(tokens[0:-1], tokens[1:len(tokens)]):\n",
    "    element = (w0, w1)\n",
    "    elements.append(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('en', 'un'),\n",
       " ('un', 'lugar'),\n",
       " ('lugar', 'de'),\n",
       " ('de', 'la'),\n",
       " ('la', 'mancha,')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al correr la siguiente celda, nos mostrar치 los pares de palabras con m치s repeticiones dentro del libro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = Counter(elements)\n",
    "bigrams.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora tenemos lo _bigram_ m치s com칰nes, 쯗e qu칠 manera podemos conocer los _bigrams_ que empiecen con ciertas palabras?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('don', 'quijote') 2149\n",
      "('don', 'fernando') 131\n",
      "('don', 'antonio') 62\n",
      "('don', 'luis') 36\n"
     ]
    }
   ],
   "source": [
    "topv = sorted(filter(lambda w: w[0] == \"don\", bigrams),\n",
    "                     key=lambda w: bigrams[w])[:-5:-1]\n",
    "for v in topv:\n",
    "    print(v, bigrams[v])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "쮻e qu칠 manera podr칤amos calcular la probabilidad de que Cervantes haya escrito `\"quijote\"` dado que la palabra precedente a esta es `\"don\"`?\n",
    "\n",
    "$$\n",
    "    \\mathbb{P}(\\texttt{\"quijote\"} | \\texttt{\"don\"})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9981421272642824\n"
     ]
    }
   ],
   "source": [
    "topv = sorted(filter(lambda w: w[1] == \"quijote\", bigrams),\n",
    "                     key=lambda w: bigrams[w])\n",
    "\n",
    "wfreq = 0\n",
    "count = 0\n",
    "for v in topv:\n",
    "    if v[0] == \"don\":\n",
    "        wfreq = bigrams[v]\n",
    "    count += bigrams[v]\n",
    "print(wfreq / count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = []\n",
    "for ws in zip(tokens[0:-1], tokens[1:len(tokens)]):\n",
    "    element = (w0, w1)\n",
    "    elements.append(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dichos pares ordenados de palabras dentro de un texto se conocen como _bigrams_. En caso de tener tercias de palabras, estos se conocen como _trigrams_.\n",
    "\n",
    "En general,  $n$ palabras ordenadas dentro un texto se conocen como $n$-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalizaci칩n\n",
    "Al igual que con las matem치ticas, la generalizaci칩n de un problema es de suma importancia. En el caso de la programaci칩n, podemos generalizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ngrams(tokens, ngram=2):\n",
    "    ntokens = len(tokens)\n",
    "    groups = [\n",
    "        tokens[slice(i, ntokens - ngram + i )]\n",
    "    for i in range(ngram)]\n",
    "    grams = [ws for ws in zip(*groups)]\n",
    "    return grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('en', 'un', 'lugar', 'de', 'la')\n",
      "('un', 'lugar', 'de', 'la', 'mancha')\n",
      "('lugar', 'de', 'la', 'mancha', 'de')\n",
      "('de', 'la', 'mancha', 'de', 'cuyo')\n",
      "('la', 'mancha', 'de', 'cuyo', 'nombre')\n"
     ]
    }
   ],
   "source": [
    "for ws in make_ngrams(tokens, ngram=5)[:5]:\n",
    "    print(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('el', 'caballero', 'de', 'la', 'triste', 'figura'), 21),\n",
       " (('caballero', 'don', 'quijote', 'de', 'la', 'mancha'), 16),\n",
       " (('de', 'don', 'quijote', 'de', 'la', 'mancha'), 16),\n",
       " (('senor', 'don', 'quijote', 'de', 'la', 'mancha'), 16),\n",
       " (('la', 'sin', 'par', 'dulcinea', 'del', 'toboso'), 14),\n",
       " (('todos', 'los', 'dias', 'de', 'mi', 'vida'), 14),\n",
       " (('dijo', 'a', 'esta', 'sazon', 'don', 'quijote'), 13),\n",
       " (('don', 'quijote', 'de', 'la', 'mancha', 'que'), 12),\n",
       " (('don', 'quijote', 'de', 'la', 'mancha', 'y'), 11),\n",
       " (('en', 'todos', 'los', 'dias', 'de', 'mi'), 10)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g5 = Counter(make_ngrams(tokens, ngram=6))\n",
    "g5.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topv = sorted(filter(lambda w: w[0] == \"mi\" and w[1] == \"perro\", g5),\n",
    "                     key=lambda w: g5[w])[:-5:-1]\n",
    "for v in topv:\n",
    "    print(v, g5[v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['muerta y medio comida de perros y picada de grajos ',\n",
       " 'entro el cura de la perroquia y tomando a los ',\n",
       " 'y desatinada en poder destos perros naturales enemigos nuestros maldita ',\n",
       " 'otros como hacen a los perros cuando en pendencia estan ',\n",
       " 'sucedio pues que entre los perros que descargo la carga ',\n",
       " 'guarda en efeto todos cuantos perros topaba aunque fuesen alanos ',\n",
       " 'el lugar sino ladridos de perros que atronaban los oidos ',\n",
       " 'le vea yo comido de perros que asi nos trae ',\n",
       " 'ciguenas el cristel de los perros el vomito y el ',\n",
       " 'de que color serian los perros que pariese a lo ',\n",
       " 'por el ladrido de los perros como por el son ',\n",
       " 'suyos cuando acosado de los perros y seguido de los ',\n",
       " 'su propio dinero dos famosos perros para guardar el ganado ']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"(?:[a-z]+\\s){1,5}perros?(?:[a-z]+\\s){1,5}\", corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('en', 'un', 'lugar', 'de', 'la', 'mancha', 'de', 'cuyo', 'nombre', 'no')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = make_ngrams(tokens, ngram=10)\n",
    "sequences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelaci칩n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8127, 21133, 13116, ...,  5904, 14679, 14659],\n",
       "       [21133, 13116,  6023, ..., 14679, 14659, 17320],\n",
       "       [13116,  6023, 12349, ..., 14659, 17320,   461],\n",
       "       ...,\n",
       "       [22035, 20983, 22034, ..., 20498, 19427,  7773],\n",
       "       [20983, 22034, 11112, ..., 19427,  7773,  1265],\n",
       "       [22034, 11112,  6023, ...,  7773,  1265, 21227]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = sorted(list(set(tokens)))\n",
    "vocab_size = len(words)\n",
    "word_ix = {w:i for i, w in enumerate(words)}\n",
    "sequences_int = [[word_ix[word] for word in seq] for seq in sequences]\n",
    "sequences_int = np.array(sequences_int)\n",
    "sequences_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(376471, 10)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_int.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = sequences_int[:,:-1], sequences_int[:, -1:]\n",
    "\n",
    "X_train = to_categorical(X_train, num_classes=vocab_size)\n",
    "y_train = to_categorical(y_train, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 9, 22147)          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               8899200   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 22147)             2236847   \n",
      "=================================================================\n",
      "Total params: 11,136,047\n",
      "Trainable params: 11,136,047\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "X_input = Input(X_train.shape[1:])\n",
    "X = LSTM(100, activation=\"relu\", return_sequences=False)(X_input)\n",
    "X = Dense(vocab_size, activation=\"softmax\")(X)\n",
    "model = Model(inputs=X_input, outputs=X)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=0.0001)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> despues de haber vivido su primera aventura don quijote se sentia meror칩 eo d칤as esegude acorras no en todo en esas a lo hecho y en entandero el de si mierda prosiba\n",
    "\n",
    "> despues de haber vivido su primera aventura don quijote se sential a vaneras dalarr a trandarle de de veno caballero alguna f치lque en contore que nocer estandoro pue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
